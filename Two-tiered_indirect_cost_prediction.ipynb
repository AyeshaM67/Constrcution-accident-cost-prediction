{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "\n",
    "import missingno as msno\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import base\n",
    "from collections import defaultdict\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "from joblib import dump, load\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, \\\n",
    "RandomizedSearchCV, cross_val_score, RepeatedStratifiedKFold, KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data.xlsx\",sheet_name='Sheet1', usecols=\"A:K\")\n",
    "#converting from korean won to us dollar\n",
    "df['Direct_insurance_costs']= df['Direct_insurance_costs']/1337\n",
    "df['Indirect_insurance_costs']= df['Indirect_insurance_costs']/1337\n",
    "df=df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column=['Human_damage_death','Human_damage_injuries','Construction_type_classification_code','Work_process_classification','Injury_area_Classification','Accident_type_classification','Workers_affiliation','Integrated_occupation_classification','Direct_insurance_costs','Direct_insurance_cost_total_category_code','Total_loss_cost_classification_code']\n",
    "df1=df[column]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unclassified total cost type from dataset\n",
    "df1 = df1[df1.Total_loss_cost_classification_code != 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# example of oversampling a multi-class classification dataset\n",
    "from pandas import read_csv\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce\n",
    "from xgboost import XGBClassifier\n",
    "X = df1.drop(columns=['Total_loss_cost_classification_code'])\n",
    "y = df1['Total_loss_cost_classification_code']\n",
    "encoder = ce.OrdinalEncoder(cols=['Construction_type_classification_code','Work_process_classification','Injury_area_Classification','Accident_type_classification','Workers_affiliation','Integrated_occupation_classification','Direct_insurance_cost_total_category_code'])\n",
    "\n",
    "\n",
    "X = encoder.fit_transform(X)\n",
    "\n",
    "X = encoder.transform(X)\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) \n",
    "def model_eval(clf, X_test=X_test, y_test=y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Mean cross-validated score of the best_estimator: {0:.3f}\".format(clf.best_score_))\n",
    "    print(\"Accuracy on train data: {0:.3f}\".format(clf.score(X_train, y_train)))\n",
    "    print(\"Accuracy on test data: {0:.3f}\".format(clf.score(X_test, y_test)))\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(\"Tuned Model Parameters: {}\".format(clf.best_params_))\n",
    "param_grids = {\n",
    "    'knn': {\n",
    "        'sampling': [None, RandomOverSampler(), RandomUnderSampler()],\n",
    "        'classifier__n_neighbors': [2, 3, 5],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    'xgb': {\n",
    "        'sampling': [None, RandomOverSampler(), RandomUnderSampler()],\n",
    "        'classifier__objective': ['reg:logistic'],\n",
    "        'classifier__max_depth': [4],\n",
    "        'classifier__alpha': [10],\n",
    "        'classifier__learning_rate': [0.01],\n",
    "        'classifier__n_estimators': [100]\n",
    "    },\n",
    "    'dt': {\n",
    "        'sampling': [None, RandomOverSampler(), RandomUnderSampler()],\n",
    "        'classifier__criterion': ['gini', 'entropy'],\n",
    "        'classifier__min_samples_split': [2, 10],\n",
    "        'classifier__max_depth': [5, 6, 7, 8, 9],\n",
    "        'classifier__min_samples_leaf': [1, 10],\n",
    "        'classifier__max_leaf_nodes': [10, 20]\n",
    "    },\n",
    "    'rf': {\n",
    "        'sampling': [None, RandomOverSampler(), RandomUnderSampler()],\n",
    "        'classifier__criterion': ['gini', 'entropy'],\n",
    "        'classifier__min_samples_split': [8, 16, 20],\n",
    "        'classifier__max_depth': [6, 8, 10, 12],\n",
    "        'classifier__min_samples_leaf': [8, 12, 18],\n",
    "        'classifier__max_leaf_nodes': [10, 20],\n",
    "        'classifier__n_estimators': [10, 100, 200]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'xgb': XGBClassifier(),\n",
    "    'dt': DecisionTreeClassifier(),\n",
    "    'rf': RandomForestClassifier()\n",
    "}\n",
    "best_models = {}\n",
    "for model_name in models:\n",
    "    print(f\"Performing Grid Search for {model_name.upper()}\")\n",
    "    pipeline = Pipeline([\n",
    "        ('sampling', 'passthrough'),  # Placeholder for sampling method\n",
    "        ('classifier', models[model_name])\n",
    "    ])\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grids[model_name], cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = {\n",
    "        'best_estimator': grid_search.best_estimator_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_\n",
    "    }\n",
    "    print(f\"Best parameters for {model_name.upper()}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {model_name.upper()}: {grid_search.best_score_}\")\n",
    "  \n",
    "    print(f\"Performing evaluation for {model_name.upper()}\")\n",
    "    model_eval(grid_search)\n",
    "\n",
    "best_model_name = max(best_models, key=lambda x: best_models[x]['best_score'])\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name.upper()} with a score of {best_model['best_score']}\")\n",
    "print(f\"Best parameters: {best_model['best_params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
    "ss_X = StandardScaler()\n",
    "ss_y = StandardScaler()\n",
    "\n",
    "df\n",
    "categorical_cols = ['Direct_insurance_cost_total_category_code','Total_loss_cost_classification_code']\n",
    "numerical_cols = ['Human_damage_death','Human_damage_injuries','Direct_insurance_costs']\n",
    "target_col = 'Indirect_insurance_costs'\n",
    "\n",
    "X = df[categorical_cols + numerical_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "# Encode categorical features\n",
    "encoder = OrdinalEncoder()\n",
    "X.loc[:, categorical_cols] = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Bin the target variable for stratification\n",
    "y_binned = pd.qcut(y, q=5, labels=False)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test, y_train_binned, y_test_binned = train_test_split(\n",
    "    X, y, y_binned, test_size=0.2, random_state=42, stratify=y_binned\n",
    ")\n",
    "\n",
    "# Standardize the features and target\n",
    "ss_X = StandardScaler()\n",
    "ss_y = StandardScaler()\n",
    "\n",
    "X_train = ss_X.fit_transform(X_train)\n",
    "X_test = ss_X.transform(X_test)\n",
    "y_train = ss_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test = ss_y.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Define models and their hyperparameters for grid search\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "    'LGBMRegressor': lgb.LGBMRegressor()\n",
    "}\n",
    "\n",
    "# Define parameter grids with correct prefixes\n",
    "params = {\n",
    "    'RandomForestRegressor': {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__max_depth': [None, 10, 20],\n",
    "        'regressor__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__max_depth': [3, 5, 7],\n",
    "        'regressor__learning_rate': [0.01, 0.05, 0.1]\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'regressor__max_depth': [None, 10, 20],\n",
    "        'regressor__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'LGBMRegressor': {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__max_depth': [10, 20],\n",
    "        'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'regressor__num_leaves': [31, 63, 127],\n",
    "        'regressor__verbose': [-1]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    'R2': make_scorer(r2_score),\n",
    "    'MAE': make_scorer(mean_absolute_error),\n",
    "    'MSE': make_scorer(mean_squared_error)\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation for each model\n",
    "best_score = float('-inf')\n",
    "best_model = None\n",
    "best_params = None\n",
    "best_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    kf = KFold(n_splits=5)  # Use KFold for regression\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=params[name], cv=kf, scoring=scoring, refit='R2')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    cv_results = grid_search.cv_results_\n",
    "    best_index = grid_search.best_index_\n",
    "    scores = {\n",
    "        'R2': cv_results['mean_test_R2'][best_index],\n",
    "        'MAE': cv_results['mean_test_MAE'][best_index],\n",
    "        'MSE': cv_results['mean_test_MSE'][best_index]\n",
    "    }\n",
    "    \n",
    "    if scores['R2'] > best_score:\n",
    "        best_score = scores['R2']\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        best_scores = scores\n",
    "\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Best params: {best_params}\")\n",
    "print(f\"Best cross-validated scores: {best_scores}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_scaled = best_model.predict(X_test)\n",
    "y_pred = ss_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "y_test = ss_y.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "test_scores = {\n",
    "    'R2': r2_score(y_test, y_pred),\n",
    "    'MAE': mean_absolute_error(y_test, y_pred),\n",
    "    \n",
    "}\n",
    "\n",
    "print(f\"Test set scores: {test_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Decalring numpy array variable\n",
    "\n",
    "xAxis = np.arange(y_test.shape[0])\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt.title(\" Regressor Result\")\n",
    "plt.xlabel(\"Test sample\")\n",
    "plt.ylabel(\"Indirect cost (US$)\")\n",
    "plt.plot(xAxis, y_pred, color =\"red\")\n",
    "plt.plot(xAxis, y_test, color =\"black\",linestyle='dotted')\n",
    "plt.legend(['prediction', 'actual']);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
